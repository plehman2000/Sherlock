{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import spacy\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "openai.organization = \"org-xYSZAaZqytg7EQ0juDkS4z5D\"\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "\n",
    "def get_ents(text):\n",
    "    #show entities in text\n",
    "    exclusionList = ['TIME', 'DATE', 'CARDINAL', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'PRODUCT']\n",
    "    doc = nlp(text)\n",
    "    entities = []\n",
    "    if doc.ents:\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_  in exclusionList:\n",
    "                pass\n",
    "            else:\n",
    "                entities.append([ent.text, ent.label_])\n",
    " \n",
    "    if entities:\n",
    "        return entities\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "def stampsToEnts(srt):\n",
    "    #associate time stamps with entities\n",
    "    stampList = []\n",
    "    for dict in srt:\n",
    "        stampList.append([dict['text'], dict['start']])\n",
    "\n",
    "    entList = []\n",
    "    for text, start in stampList:\n",
    "        entList.append([get_ents(text), start])\n",
    "\n",
    "    #return entList\n",
    "\n",
    "    entStamped = []\n",
    "    for ents, time in entList:\n",
    "        if ents != 0:\n",
    "            entStamped.append([ents, time])\n",
    "    return entStamped\n",
    "\n",
    "def getFullScript(srt):\n",
    "    #turn timestamped transcript into string\n",
    "    script = \"\"\n",
    "    for dict in srt:\n",
    "        script += dict['text'] + \" \"\n",
    "    return script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag Meanings\n",
    "- PERSON People, including fictional\n",
    "- NORP Nationalities or religious or political groups\n",
    "- FACILITY Buildings, airports, highways, bridges, etc.\n",
    "- ORGANIZATION Companies, agencies, institutions, etc.\n",
    "- GPE Countries, cities, states\n",
    "- LOCATION Non-GPE locations, mountain ranges, bodies of water\n",
    "- PRODUCT Vehicles, weapons, foods, etc. (Not services)\n",
    "- EVENT Named hurricanes, battles, wars, sports events, etc.\n",
    "- WORK OF ART Titles of books, songs, etc.\n",
    "- LAW Named documents made into laws \n",
    "\n",
    "- LANGUAGE Any named language\n",
    "### Excluded\n",
    "- DATE Absolute or relative dates or periods\n",
    "- TIME Times smaller than a day\n",
    "- PERCENT Percentage (including “%”)\n",
    "- MONEY Monetary values, including unit\n",
    "- QUANTITY Measurements, as of weight or distance\n",
    "- ORDINAL “first”, “second”\n",
    "- CARDINAL Numerals that do not fall under another type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[['dutch', 'NORP']], 4.08],\n",
       " [[['britain', 'GPE']], 6.319],\n",
       " [[['uk', 'GPE']], 17.76],\n",
       " [[['uk', 'GPE']], 19.92],\n",
       " [[['english', 'LANGUAGE']], 30.56],\n",
       " [[['honduras', 'GPE']], 37.44],\n",
       " [[['uk', 'GPE']], 40.16],\n",
       " [[['british', 'NORP']], 43.44],\n",
       " [[['dutch', 'NORP']], 58.399],\n",
       " [[['uk', 'GPE']], 78.24],\n",
       " [[['the european union', 'ORG']], 99.2],\n",
       " [[['edith', 'PERSON']], 111.68],\n",
       " [[['britain', 'GPE']], 119.439],\n",
       " [[['uk', 'GPE']], 137.84]]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srt = getTranscriptFromURL('https://www.youtube.com/watch?v=lLiCCHg6q6A')\n",
    "\n",
    "a = stampsToEnts(srt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ent_ts_pairs(entities_from_transcript):\n",
    "    ent_ts_pairs = dict()\n",
    "    for ent_list in entities_from_transcript:\n",
    "        entity = ent_list[0][0][0]\n",
    "        timestamp = ent_list[1]\n",
    "        \n",
    "        if entity not in ent_ts_pairs.keys():\n",
    "            ent_ts_pairs[entity] = []\n",
    "        ent_ts_pairs[entity].append(timestamp)\n",
    "    return ent_ts_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = get_ent_ts_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process\n",
    "def fuzzy_search(query, ent_ts_pairs, limit=-1):\n",
    "    \"\"\"\n",
    "    Returns a dictionary with key = query word and value = list of tuples,\n",
    "    with keyword and relevance as first and second values in tuples respectively.\n",
    "    \"\"\"\n",
    "    ent_ts_pairs_sorted = []\n",
    "    entities_list = ent_ts_pairs.keys()\n",
    "    process_limit = len(entities_list)\n",
    "    if limit != -1:\n",
    "        assert limit > 0 and type(limit) is int, \"Limit should be an integer greater than 0.\"\n",
    "        process_limit = limit\n",
    "        \n",
    "    keywords = query.split(\";\")\n",
    "    sorted_dict = {}\n",
    "\n",
    "    for word in keywords:\n",
    "        sorted_dict[word] = process.extract(word, entities_list, limit=process_limit)\n",
    "        \n",
    "    kw_rel_ts_sorted_dict = dict()\n",
    "    for kw, kw_relevance_tuples_list in sorted_dict.items():\n",
    "        kw_rel_ts_list = []\n",
    "        for kw_relevance_tuple in kw_relevance_tuples_list:\n",
    "            curr_entity = kw_relevance_tuple[0]\n",
    "            kw_rel_ts = [curr_entity, kw_relevance_tuple[1], ent_ts_pairs[curr_entity]]\n",
    "            kw_rel_ts_list.append(kw_rel_ts)\n",
    "    kw_rel_ts_sorted_dict[kw] = kw_rel_ts_list\n",
    "    \n",
    "    return kw_rel_ts_sorted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def getIDFromURL(youtube_url):\n",
    "    pattern = r'(?:https?:\\/\\/)?(?:[0-9A-Z-]+\\.)?(?:youtube|youtu|youtube-nocookie)\\.(?:com|be)\\/(?:watch\\?v=|watch\\?.+&v=|embed\\/|v\\/|.+\\?v=)?([^&=\\n%\\?]{11})'\n",
    "    result = re.findall(pattern, youtube_url, re.IGNORECASE)\n",
    "    \n",
    "    return result[0]\n",
    "\n",
    "def getURLFromQuery(query, vid_url):\n",
    "    \n",
    "    vid_id = getIDFromURL(vid_url)\n",
    "    base_ts_url = f\"https://youtu.be/{vid_id}?t=\"\n",
    "    transcript = getTranscriptFromURL(vid_url)\n",
    "    entities_from_transcript = stampsToEnts(transcript)\n",
    "    \n",
    "    ent_ts_pairs = get_ent_ts_pairs(entities_from_transcript)\n",
    "    kw_rel_ts_sorted_dict = fuzzy_search(query, ent_ts_pairs)\n",
    "    \n",
    "    entity_tslink_dict = {}\n",
    "    for kw in kw_rel_ts_sorted_dict.keys():\n",
    "        tslink_list = []\n",
    "        for kw_rel_ts_list in kw_rel_ts_sorted_dict[entity]:\n",
    "            curr_entity = kw_rel_ts_list[0]\n",
    "            for ts in kw_rel_ts_list[3]:\n",
    "                print(kw_rel_ts)\n",
    "                tslink = base_ts_url + str(round(float(ts)))\n",
    "                tslink_list.append(ts_link)\n",
    "            entity_tslink_dict[curr_entity] = tslink_list\n",
    "    \n",
    "    return entity_ts_link_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'uk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-156-a35c5368995d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://www.youtube.com/watch?v=lLiCCHg6q6A\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgetURLFromQuery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"brexit\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-155-7aec4f3929f4>\u001b[0m in \u001b[0;36mgetURLFromQuery\u001b[1;34m(query, vid_url)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkw_rel_ts_sorted_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mtslink_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mkw_rel_ts_list\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkw_rel_ts_sorted_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m             \u001b[0mcurr_entity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkw_rel_ts_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mts\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkw_rel_ts_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'uk'"
     ]
    }
   ],
   "source": [
    "test_url = \"https://www.youtube.com/watch?v=lLiCCHg6q6A\"\n",
    "getURLFromQuery(\"brexit\", test_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getTranscriptFromURL(vidcode):\n",
    "    #gets timestamped transcript from URL\n",
    "    vidcode = vidcode[vidcode.find('v='):][2:]\n",
    "    if (vidcode[vidcode.find('&t')]):\n",
    "        vidcode = vidcode.split('&')[0]\n",
    "        try:\n",
    "            srt = YouTubeTranscriptApi.get_transcript(vidcode)\n",
    "            return srt\n",
    "        except:\n",
    "            return -1\n",
    "    \n",
    "\n",
    "def getSummary(textList, engineChoice=0, sentences_per_chunk=3):\n",
    "    #Summarizes list of strings using openai api\n",
    "    #Running this function costs me money so please don't unless necessary for testing/compatibility purposes - Patrick\n",
    "    engineChoices = ['text-davinci-001', 'text-curie-001']\n",
    "    summaries = []\n",
    "    for text_to_summarize in textList: \n",
    "        #sentence_amount = int(len(text_to_summarize.split()) / (3*15))\n",
    "        text_to_summarize = text_to_summarize + \".\" #to prevent period at the start of output\n",
    "        if (sentences_per_chunk * 32) > 2049:\n",
    "            token_amount = 2049 \n",
    "        else:\n",
    "            token_amount = sentences_per_chunk * 20\n",
    "        #return [f\"Summarize this for a second-grade student and write only {sentence_amount} sentences:\\n\" + text_to_summarize, token_amount]\n",
    "        comp =  openai.Completion.create(\n",
    "        engine=engineChoices[engineChoice],\n",
    "        prompt=f\"Summarize this for a second-grade student and write only {sentences_per_chunk} sentences:\\n\" + text_to_summarize,\n",
    "        max_tokens=token_amount,\n",
    "        temperature=0.5\n",
    "        )\n",
    "        summaries.append((comp.choices[0]['text']).replace('\\n', \"\"))\n",
    "    return summaries\n",
    "    \n",
    "def splitTranscript(script_string, limit=910):\n",
    "    #Splits transcripts string into list of chunks of length 'limit\n",
    "    tokenCount = len(script_string.split(' '))\n",
    "    numSplits = int(tokenCount/limit)\n",
    "    transcript_list = []\n",
    "\n",
    "    splitString = script_string.split(' ')\n",
    "    #print(numSplits)\n",
    "    #print(f'splitstring: {splitString}')\n",
    "    ind = 0\n",
    "    for ind in range(0,numSplits*limit, limit):\n",
    "        if ind != numSplits:\n",
    "            script = splitString[ind:ind+limit]\n",
    "            #print(script)\n",
    "            transcript_list.append(  ' '.join(script))\n",
    "        else:\n",
    "            break\n",
    "    last_slice = splitString[ind+limit:]\n",
    "    last_script = ' '.join(last_slice )\n",
    "    transcript_list.append(last_script)\n",
    "    return transcript_list\n",
    "\n",
    "\n",
    "def getSummaryFromURL(URL, token_limit=896):\n",
    "    #Running this function costs me money so please don't unless necessary for testing/compatibility purposes - Patrick\n",
    "    transcript = getTranscriptFromURL(URL) #vid on debt crisis\n",
    "    script_string = [getFullScript(transcript)]\n",
    "    if len(script_string[0].split(' ')) > token_limit:\n",
    "        script_string = splitTranscript(script_string[0], token_limit)\n",
    "\n",
    "    summary = getSummary(script_string)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"The GeForce RTX 3050 is a good entry-level GPU. It's more expensive than some consoles, but it's also more powerful. It's a good choice if you need a new GPU today and don't mind 1080p resolution.\", 'The Radeon RX 6500 XT is more efficient than the Nvidia RTX 3050, but it is also more expensive. The GTX 1660 Super is more affordable, but it does not perform as well as the RX 6500 XT.']\n"
     ]
    }
   ],
   "source": [
    "summary = getSummaryFromURL('https://www.youtube.com/watch?v=SlobQZECwxE')\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "797a258c5e0138499813c5bcd8925e67ef79289af20fce8701ba95a0c64ae90e"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
